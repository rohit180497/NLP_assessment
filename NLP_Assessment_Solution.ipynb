{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score ,confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset\n",
    "\n",
    "- Load the train data and using all your knowledge try to explore the different statistical properties of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e    122013\n",
      "b     92679\n",
      "t     86846\n",
      "m     36397\n",
      "Name: CATEGORY, dtype: int64\n",
      "       Id                                              TITLE CATEGORY\n",
      "0   50846         Ukraine to get $18 billion rescue from IMF        b\n",
      "1  234375  McDonald's Abandons Headquarters to Avoid Prot...        b\n",
      "2   63422  New study finds evidence that Autism begins in...        m\n",
      "3  353942  Prime Minister Modi Says Meeting With Facebook...        t\n",
      "4  311586  New robot guides at Tokyo museum almost outper...        t\n"
     ]
    }
   ],
   "source": [
    "# Code starts here\n",
    "\n",
    "# load data\n",
    "news = pd.read_csv(r\"/Users/rahulkosamkar/Documents/Data_Science/Projects/NLP_assessment/train.csv\")\n",
    "\n",
    "# distribution of classes\n",
    "dist = news.CATEGORY.value_counts()\n",
    "\n",
    "# display class distribution\n",
    "print(dist)\n",
    "\n",
    "# display data\n",
    "print(news.head())\n",
    "\n",
    "# Code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize and Preprocess the data\n",
    "\n",
    "- Retaining only alphabets (Using regular expressions)\n",
    "- Removing stopwords (Using nltk library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rahulkosamkar/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code starts here\n",
    "\n",
    "# stopwords \n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "# retain only alphabets\n",
    "news['TITLE'] = news['TITLE'].apply(lambda x:re.sub(\"[^a-zA-Z]\", \" \",x))\n",
    "\n",
    "# convert to lowercase and tokenize\n",
    "news['TITLE'] = news['TITLE'].apply(lambda x:x.lower().split())\n",
    "\n",
    "# remove stopwords\n",
    "news['TITLE'] = news['TITLE'].apply(lambda x:[i for i in x if i not in stop])\n",
    "\n",
    "# join list elements\n",
    "news['TITLE'] = news['TITLE'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# split into training and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(news[\"TITLE\"], news[\"CATEGORY\"],test_size = 0.2,random_state=3)\n",
    "\n",
    "# Code ends here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code starts here\n",
    "\n",
    "# initialize count vectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# initialize tfidf vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n",
    "\n",
    "# fit and transform with count vectorizer\n",
    "X_train_count = count_vectorizer.fit_transform(X_train)\n",
    "X_test_count = count_vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "# fit and transform with tfidf vectorizer\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Code ends here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(270348,) (270348,)\n",
      "(67587,) (67587,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(270348, 42616) (270348,)\n",
      "(67587, 1665439) (67587,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_count.shape, Y_train.shape)\n",
    "print(X_test_tfidf.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model building\n",
    "\n",
    "- Now let's come to the actual task, using any classifier, predict the `CATEGORY`. Use different techniques you have learned to imporove the performance of the model.\n",
    "- Try improving upon the `accuracy_score` ([Accuracy Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9272641188394218 0.9295870507642002\n"
     ]
    }
   ],
   "source": [
    "# Code starts here\n",
    "\n",
    "# initialize multinomial naive bayes\n",
    "nb_1 = MultinomialNB()\n",
    "nb_2 = MultinomialNB()\n",
    "\n",
    "# fit on count vectorizer training data\n",
    "nb_1.fit(X_train_count, Y_train)\n",
    "\n",
    "# fit on tfidf vectorizer training data\n",
    "nb_2.fit(X_train_tfidf, Y_train)\n",
    "\n",
    "# accuracy with count vectorizer\n",
    "acc_count_nb = accuracy_score(nb_1.predict(X_test_count), Y_test)\n",
    "\n",
    "# accuracy with tfidf vectorizer\n",
    "acc_tfidf_nb = accuracy_score(nb_2.predict(X_test_tfidf), Y_test)\n",
    "\n",
    "# display accuracies\n",
    "print(acc_count_nb, acc_tfidf_nb)\n",
    "\n",
    "# Code ends here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9461434891325254 0.9420598635832335\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# initialize logistic regression\n",
    "logreg_1 = OneVsRestClassifier(LogisticRegression(random_state=10))\n",
    "logreg_2 = OneVsRestClassifier(LogisticRegression(random_state=10))\n",
    "\n",
    "# fit on count vectorizer training data\n",
    "logreg_1.fit(X_train_count, Y_train)\n",
    "\n",
    "# fit on tfidf vectorizer training data\n",
    "logreg_2.fit(X_train_tfidf, Y_train)\n",
    "\n",
    "# accuracy with count vectorizer\n",
    "acc_count_logreg = accuracy_score(logreg_1.predict(X_test_count), Y_test)\n",
    "\n",
    "# accuracy with tfidf vectorizer\n",
    "acc_tfidf_logreg = accuracy_score(logreg_2.predict(X_test_tfidf), Y_test)\n",
    "\n",
    "# display accuracies\n",
    "print(acc_count_logreg, acc_tfidf_logreg)\n",
    "\n",
    "# Code ends here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83168     m\n",
      "10859     e\n",
      "303271    e\n",
      "127183    e\n",
      "94932     e\n",
      "Name: CATEGORY, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(Y_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((67587,), (67587,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_2.predict(X_test_tfidf).shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on the test data and creating the sample submission file.\n",
    "\n",
    "- Load the test data and store the `Id` column in a separate variable.\n",
    "- Perform the same operations on the test data that you have performed on the train data.\n",
    "- Create the submission file as a `csv` file consisting of the `Id` column from the test data and your prediction as the second column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84484, 42616)\n",
      "       Id CATEGORY\n",
      "0   86998        m\n",
      "1  112926        t\n",
      "2  280943        m\n",
      "3   37154        m\n",
      "4  152800        t\n"
     ]
    }
   ],
   "source": [
    "# Code Starts here\n",
    "# Prediction on test data\n",
    "\n",
    "# Read the test data\n",
    "test = pd.read_csv(r'/Users/rahulkosamkar/Documents/Data_Science/Projects/NLP_assessment/test.csv')\n",
    "\n",
    "# Storing the id from the test file\n",
    "id_ = test['Id']\n",
    "\n",
    "# Apply the transformations on test\n",
    "# retain only alphabets\n",
    "test['TITLE'] = test['TITLE'].apply(lambda x:re.sub(\"[^a-zA-Z]\", \" \",x))\n",
    "\n",
    "# convert to lowercase and tokenize\n",
    "test['TITLE'] = test['TITLE'].apply(lambda x:x.lower().split())\n",
    "\n",
    "# remove stopwords\n",
    "test['TITLE'] = test['TITLE'].apply(lambda x:[i for i in x if i not in stop])\n",
    "\n",
    "# join list elements\n",
    "test['TITLE'] = test['TITLE'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "\n",
    "test_count = count_vectorizer.transform(test['TITLE'])\n",
    "test_tfidf = tfidf_vectorizer.transform(test['TITLE'])\n",
    "\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_test = logreg_1.predict(test_count)\n",
    "print(test_count.shape)\n",
    "y_pred_test = y_pred_test.flatten()\n",
    "\n",
    "# Create a sample submission file\n",
    "sample_submission = pd.DataFrame({'Id':id_,'CATEGORY':y_pred_test})\n",
    "print(sample_submission.head())\n",
    "\n",
    "# Convert the sample submission file into a csv file\n",
    "sample_submission.to_csv('sample_submission.csv',index=False)\n",
    "\n",
    "# Code ends here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
